# æ•°å­—äººåŒå·¥é€šä¿¡åè®®è§„èŒƒ

## ğŸ¯ æ¦‚è¿°

æœ¬æ–‡æ¡£å®šä¹‰äº†æ•°å­—äººå¯¹è¯ç³»ç»Ÿä¸­å‰ç«¯ã€åç«¯WebSocketé€‚é…å™¨å’Œå¤§æ¨¡å‹ç«¯ä¹‹é—´å®Œæ•´çš„åŒå·¥é€šä¿¡åè®®è§„èŒƒï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶çš„æ¶ˆæ¯æ ¼å¼ã€å­—æ®µå‘½åå’ŒJSONç»“æ„å®Œå…¨ç»Ÿä¸€ã€‚

## ğŸ“‹ ç»„ä»¶æ¶æ„

```
å‰ç«¯ (React Native)
    â†“ WebSocket
åç«¯é€‚é…å™¨ (websocket_llm_adapter.py) 
    â†“ æœ¬åœ°æ¨ç†
å¤§æ¨¡å‹å¤„ç†å™¨ (LLMProcessor)
    
æˆ–è€…

å‰ç«¯ (React Native)
    â†“ WebSocket  
LLMå“åº”æ¥å£ (llm_response.py)
    â†“ WebSocket
è¿œç¨‹å¤§æ¨¡å‹æœåŠ¡å™¨
```

## ğŸ”Œ WebSocketè¿æ¥é…ç½®

### è¿æ¥ç«¯ç‚¹
- **ä¸»æœåŠ¡å™¨**: `ws://10.91.225.137:8000`
- **æœ¬åœ°å¼€å‘**: `ws://localhost:8000`
- **åè®®**: WebSocket (ws://)
- **è¶…æ—¶è®¾ç½®**: 60ç§’
- **é‡è¿æœºåˆ¶**: æœ€å¤š10æ¬¡é‡è¯•ï¼Œ3ç§’é—´éš”

### è¿æ¥å‚æ•°
```json
{
  "timeout": 60000,
  "reconnectAttempts": 10,
  "reconnectDelay": 3000,
  "pingInterval": 20000,
  "maxMessageSize": 1048576
}
```

## ğŸ“¨ æ¶ˆæ¯æ ¼å¼è§„èŒƒ

### å­—æ®µå‘½åè§„åˆ™
- **é¡¶çº§å­—æ®µ**: camelCase (`requestId`, `timestamp`)
- **dataå†…å­—æ®µ**: snake_case (`prompt`, `system_prompt`, `conversation_history`, `max_tokens`)
- **æ¶ˆæ¯ç±»å‹**: snake_case (`llm_request`, `llm_response`)
- **æ—¶é—´æˆ³**: JavaScriptæ¯«ç§’æ ¼å¼ (`Date.now()` / `int(time.time() * 1000)`)

### JSONç¼–ç è§„èŒƒ
- **Pythonç«¯**: `json.dumps(message, ensure_ascii=False)`
- **JavaScriptç«¯**: `JSON.stringify(message)`
- **å­—ç¬¦ç¼–ç **: UTF-8
- **ä¸­æ–‡æ”¯æŒ**: å®Œæ•´æ”¯æŒ

## ğŸ”„ å®Œæ•´æ¶ˆæ¯æµç¨‹

### 1. å‰ç«¯å‘é€LLMè¯·æ±‚

**å‘é€æ–¹**: React Nativeå‰ç«¯ (`ResponseLLMService.js`)
**æ¥æ”¶æ–¹**: åç«¯WebSocketé€‚é…å™¨

```json
{
  "type": "llm_request",
  "requestId": 123,
  "data": {
    "prompt": "ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯å†…å®¹",
    "system_prompt": "ä½ å«åƒé—®ï¼Œæ˜¯ä¸€ä¸ª18å²çš„å¥³å¤§å­¦ç”Ÿï¼Œæ€§æ ¼æ´»æ³¼å¼€æœ—ï¼Œè¯´è¯ä¿çš®",
    "conversation_history": [
      {"role": "user", "content": "ä¹‹å‰çš„ç”¨æˆ·æ¶ˆæ¯"},
      {"role": "assistant", "content": "ä¹‹å‰çš„AIå›å¤"}
    ],
    "max_tokens": 512
  },
  "timestamp": 1672531200000
}
```

**å‰ç«¯ä»£ç å®ç°**:
```javascript
const requestData = {
  type: 'llm_request',
  requestId: ++this.requestId,
  data: {
    prompt: userInput,
    conversation_history: conversationHistory,
    max_tokens: this.modelConfig.max_tokens,
    system_prompt: llmConfig.gabalong.system_prompt,
  },
  timestamp: Date.now(),
}
```

### 2. åç«¯å¤„ç†å’Œå“åº”

**å¤„ç†æ–¹**: WebSocketé€‚é…å™¨ (`websocket_llm_adapter.py`) / LLMå“åº”æ¥å£ (`llm_response.py`)

**å­—æ®µæå–**:
```python
request_id = data.get("requestId")
request_data = data.get("data", {})
prompt = request_data.get("prompt", "")
system_prompt = request_data.get("system_prompt")
conversation_history = request_data.get("conversation_history", [])
max_tokens = request_data.get("max_tokens", 512)
```

### 3. å¤§æ¨¡å‹æˆåŠŸå“åº”

**å‘é€æ–¹**: åç«¯WebSocketé€‚é…å™¨
**æ¥æ”¶æ–¹**: React Nativeå‰ç«¯

```json
{
  "type": "llm_response",
  "requestId": 123,
  "success": true,
  "message": "ä½ å¥½ï¼æˆ‘æ˜¯åƒé—®ï¼Œå¾ˆé«˜å…´å’Œä½ èŠå¤©ï¼",
  "timestamp": 1672531201000
}
```

**åç«¯ä»£ç å®ç°**:
```python
response = {
    "type": "llm_response",
    "requestId": request_id,
    "success": result["success"],
    "timestamp": int(time.time() * 1000)
}
if result["success"]:
    response["message"] = result["message"]
else:
    response["error"] = result["error"]
```

### 4. å¤§æ¨¡å‹å¤±è´¥å“åº”

```json
{
  "type": "llm_response",
  "requestId": 123,
  "success": false,
  "error": "æ¨¡å‹å¤„ç†å¤±è´¥: è¾“å…¥è¿‡é•¿",
  "timestamp": 1672531201000
}
```

### 5. å‰ç«¯å“åº”å¤„ç†

**å¤„ç†æ–¹**: React Nativeå‰ç«¯ (`ResponseLLMService.js`)

```javascript
handleWebSocketMessage(data) {
  if (data.type === 'llm_response' && data.requestId) {
    const request = this.pendingRequests.get(data.requestId)
    if (request) {
      clearTimeout(request.timeoutId)
      this.pendingRequests.delete(data.requestId)
      
      if (data.success) {
        request.resolve({
          success: true,
          message: data.message,
          timestamp: data.timestamp || Date.now(),
        })
      } else {
        request.reject(new Error(data.error || 'LLM processing failed'))
      }
    }
  }
}
```

## ğŸ”§ é”™è¯¯å¤„ç†æœºåˆ¶

### é€šç”¨é”™è¯¯å“åº” (æ— requestId)

```json
{
  "type": "error",
  "error": "WebSocket connection failed",
  "timestamp": 1672531201000
}
```

### LLMç‰¹å®šé”™è¯¯å“åº” (æœ‰requestId)

```json
{
  "type": "llm_response",
  "requestId": 123,
  "success": false,
  "error": "Empty prompt provided",
  "timestamp": 1672531201000
}
```

### å‰ç«¯é”™è¯¯å¤„ç†

```javascript
} else if (data.type === 'error') {
  console.error('WebSocketé€šç”¨é”™è¯¯:', data.error)
  if (data.requestId) {
    const request = this.pendingRequests.get(data.requestId)
    if (request) {
      clearTimeout(request.timeoutId)
      this.pendingRequests.delete(data.requestId)
      request.reject(new Error(data.error || 'Server error'))
    }
  }
}
```

## ğŸ’“ å¿ƒè·³æ£€æµ‹æœºåˆ¶

### Pingæ¶ˆæ¯

```json
{
  "type": "ping",
  "timestamp": 1672531200000
}
```

### Pongå“åº”

```json
{
  "type": "pong",
  "timestamp": 1672531200001
}
```

## ğŸ“Š æ•°æ®ç»“æ„å®šä¹‰

### LLMRequestData (Python)

```python
@dataclass
class LLMRequestData:
    prompt: str
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"
    conversation_history: List[Dict[str, str]] = None
    max_tokens: int = 512
    temperature: float = 0.7
```

### LLMResponseData (Python)

```python
@dataclass
class LLMResponseData:
    success: bool
    message: str
    requestId: Union[str, int]  # ç»Ÿä¸€ä½¿ç”¨camelCase
    timestamp: float
    error: Optional[str] = None
    model_info: Optional[Dict[str, Any]] = None
    usage: Optional[Dict[str, Any]] = None
```

## ğŸ¯ æ¶ˆæ¯ç±»å‹æšä¸¾

```python
class MessageType(Enum):
    LLM_REQUEST = "llm_request"
    LLM_RESPONSE = "llm_response"
    PING = "ping"
    PONG = "pong"
    ERROR = "error"
    STATUS = "status"
```

## ğŸš€ å®Œæ•´æ•°å­—äººå¯¹è¯æµç¨‹

```
1. ç”¨æˆ·è¯­éŸ³è¾“å…¥ â†’ STTè½¬æ¢
2. å‰ç«¯å‘é€ llm_request â†’ åç«¯é€‚é…å™¨
3. åç«¯æå–å­—æ®µ â†’ å¤§æ¨¡å‹å¤„ç†
4. å¤§æ¨¡å‹ç”Ÿæˆå›å¤ â†’ åç«¯å°è£…å“åº”
5. åç«¯å‘é€ llm_response â†’ å‰ç«¯æ¥æ”¶
6. å‰ç«¯å¤„ç†å“åº” â†’ TTSè¯­éŸ³æ’­æ”¾
7. æ•°å­—äººåŠ¨ç”»æ˜¾ç¤ºå¯¹è¯å†…å®¹
```

## âœ… éªŒè¯æ¸…å•

**å­—æ®µå‘½åä¸€è‡´æ€§**:
- âœ… `requestId` (camelCase) - æ‰€æœ‰ç»„ä»¶ç»Ÿä¸€
- âœ… `prompt` (snake_case) - æ‰€æœ‰ç»„ä»¶ç»Ÿä¸€  
- âœ… `system_prompt` (snake_case) - æ‰€æœ‰ç»„ä»¶ç»Ÿä¸€
- âœ… `conversation_history` (snake_case) - æ‰€æœ‰ç»„ä»¶ç»Ÿä¸€
- âœ… `max_tokens` (snake_case) - æ‰€æœ‰ç»„ä»¶ç»Ÿä¸€

**JSONæ ¼å¼ä¸€è‡´æ€§**:
- âœ… è¯·æ±‚æ ¼å¼: å‰ç«¯å‘é€ â†’ åç«¯æ¥æ”¶
- âœ… å“åº”æ ¼å¼: åç«¯å‘é€ â†’ å‰ç«¯æ¥æ”¶  
- âœ… é”™è¯¯æ ¼å¼: ä¸¤ç§é”™è¯¯ç±»å‹æ­£ç¡®å¤„ç†
- âœ… æ—¶é—´æˆ³æ ¼å¼: ç»Ÿä¸€JavaScriptæ¯«ç§’æ ¼å¼

**ç¼–ç ä¸€è‡´æ€§**:
- âœ… Pythonç«¯: `ensure_ascii=False`
- âœ… JavaScriptç«¯: åŸç”ŸUTF-8æ”¯æŒ
- âœ… ä¸­æ–‡æ¶ˆæ¯: å®Œæ•´æ”¯æŒä¼ è¾“

## ğŸ“ æ–‡ä»¶å¯¹åº”å…³ç³»

**å‰ç«¯ç»„ä»¶**:
- `src/services/ResponseLLMService.js` - LLMè¯·æ±‚å‘é€å’Œå“åº”å¤„ç†
- `src/services/WebSocketService.js` - WebSocketè¿æ¥ç®¡ç†
- `src/services/DigitalHumanService.js` - æ•°å­—äººå¯¹è¯æµç¨‹æ§åˆ¶

**åç«¯ç»„ä»¶**:
- `response/websocket_llm_adapter.py` - WebSocketé€‚é…å™¨å’Œæœ¬åœ°LLM
- `llm_response.py` - LLMå“åº”æ¥å£ï¼ˆè¿œç¨‹æ¨¡å‹ï¼‰
- `response/start_llm_server.py` - æœåŠ¡å™¨å¯åŠ¨è„šæœ¬

**é…ç½®æ–‡ä»¶**:
- `src/config/llmConfig.js` - å‰ç«¯LLMé…ç½®
- `llm_config.yaml` - Pythonç«¯LLMé…ç½®

---

**æœ€åéªŒè¯æ—¶é—´**: 2024-08-12  
**åè®®ç‰ˆæœ¬**: 1.0  
**å…¼å®¹æ€§**: æ‰€æœ‰ç»„ä»¶å®Œå…¨å…¼å®¹ç»Ÿä¸€æ ¼å¼ âœ…